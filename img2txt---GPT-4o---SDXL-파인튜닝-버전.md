ìƒìœ„ ë¬¸ì„œë¡œ ì´ë™ : [AI Wiki](https://github.com/100-hours-a-week/16-Hot6-wiki/wiki/AI-Wiki)

# img2txt + GPT 4o + SDXL íŒŒì¸íŠœë‹ ë²„ì „
- colabì—ì„œ ì§„í–‰ L4 ê³ ìš©ëŸ‰ RAM ì‚¬ìš©í•˜ì˜€ëŠ”ë° T4ë„ ê°€ëŠ¥í•´ë³´ì„
## img2txt + GPT 4o
```python
# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import torch
from PIL import Image
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import openai

# 3. API í‚¤ ì„¤ì •
client = openai.OpenAI(api_key="OPENAI_API_KEY")  # ğŸ” ë‹¹ì‹ ì˜ OpenAI í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”

# 4. BLIP2 ëª¨ë¸ ë¡œë“œ
processor = Blip2Processor.from_pretrained("Salesforce/blip2-flan-t5-xl")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-flan-t5-xl",
    torch_dtype=torch.float16,
    device_map="auto"
)

# 5. ì´ë¯¸ì§€ ë¡œë“œ
image = Image.open("desk.jpg").convert("RGB")  # ğŸ”„ ê²½ë¡œ ìˆ˜ì •

# 6. ìº¡ì…˜ ìƒì„± (BLIP2)
inputs = processor(images=image, return_tensors="pt").to("cuda", torch.float16)
generated_ids = model.generate(**inputs, max_new_tokens=50)
caption = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)

print(f"ğŸ“ [BLIP2 Caption]: {caption}")

# GPT-4o í”„ë¡¬í”„íŠ¸ ìƒì„±
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "You are a creative prompt engineer for image generation."},
        {"role": "user", "content": f"Make this into a vivid prompt for image generation: {caption}"}
    ],
    temperature=0.8,
    max_tokens=70
)

# âœ… ê°ì²´ ì†ì„± ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼
generated_prompt = response.choices[0].message.content
print(f"ğŸ¨ [GPT-4o Prompt]: {generated_prompt}")
```

- **output**
```jsons
ğŸ“ [BLIP2 Caption]: a monitor screen is shown on a desk
ğŸ¨ [GPT-4o Prompt]: Create a detailed and visually striking scene of a modern, high-tech workspace. Center the composition around a sleek, ultra-wide monitor screen displaying vibrant graphics. The monitor is positioned on a minimalist, polished wooden desk. Surround the screen with subtle reflections and ambient lighting that highlights the contours of the monitor. Add small, realistic details such as a wireless keyboard and
```

- VRAM : 8215MiB ì‚¬ìš©

- memory ë¹„ì›Œì£¼ê¸°
```python
import gc
import torch

# ëª¨ë¸ ì‚­ì œ
del model
del processor
del inputs
del generated_ids

# ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ + ìºì‹œ ë¹„ìš°ê¸°
gc.collect()
torch.cuda.empty_cache()
```

## SDXL íŒŒì¸íŠœë‹
### base ëª¨ë¸
```python
base_model_path = "fluently/Fluently-XL-Final"
vae_path = "/content/drive/MyDrive/Lora_test/sdxl_vae_madebyollin.safetensors"
```

### íŒŒì´í”„ë¼ì¸ ì„¤ì •
```python
from diffusers import DiffusionPipeline
# íŒŒì´í”„ë¼ì¸ ì„¤ì •
pipe = DiffusionPipeline.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    # variant="fp16",
    use_safetensors=True,
)
pipe.to("cuda")

if vae_path and os.path.exists(vae_path):
    pipe.vae = AutoencoderKL.from_single_file(
        vae_path, torch_dtype=torch.float16
    ).to("cuda")
```

### íŒŒì¸íŠœë‹
```python
# lora ì„¤ì •
# pipe.load_lora_weights("loraê²½ë¡œ", weight_name="defalut", adapter_name="lora ì´ë¦„ í•˜ê³  ì‹¶ì€ê±°")
pipe.load_lora_weights("/content/drive/MyDrive/Lora_test/dalle-3-xl-lora-v2.safetensors", weight_name="default", adapter_name="base_lora")
pipe.load_lora_weights("/content/drive/MyDrive/Lora_test/ott_fluently.safetensors", weight_name="default", adapter_name="ott_lora")
pipe.load_lora_weights("/content/drive/MyDrive/Lora_test/3D_Office.safetensors", weight_name = "default", adapter_name="3d_officer")


# lora ì–´ë–»ê²Œ ì„¤ì •í• ê±´ì§€, ì—¬ëŸ¬ loraë¥¼ í•œ ë²ˆì— í…ŒìŠ¤íŠ¸ í•˜ê¸°ìœ„í•´ dict êµ¬ì¡°ë¡œ ì„¤ì •
lora_settings = {
    "base_only": (["base_lora"], [1.0]),
    "ott_only": (["ott_lora"], [1.0]),
    "3d_only": (["3d_officer"], [1.0]),
    "base_ott": (["base_lora", "ott_lora"], [0.7, 0.5]),
    "base_3d": (["base_lora", "3d_officer"], [0.7, 0.5]),
    "ott_3d": (["ott_lora", "3d_officer"], [0.7, 0.5]),
    "mix": (["base_lora", "ott_lora", "3d_officer"], [0.6, 0.4, 0.4]),
}
```

### img2txtì— ìº¡ì…˜ ë‚´ìš©ì— prompt ì¶”ê°€
- ê¸€ììˆ˜ ì œí•œìœ¼ë¡œ ì§¤ë¦¼(ì¶”í›„ ì—…ë°ì´íŠ¸í•´ë³´ë©´ì„œ ì§„í–‰í•  ì˜ˆì •)
```jsons
generated_prompt = Create a detailed and visually striking scene of a modern, high-tech workspace. Center the composition around a sleek, ultra-wide monitor screen displaying vibrant graphics. The monitor is positioned on a minimalist, polished wooden desk. Surround the screen with subtle reflections and ambient lighting that highlights the contours of the monitor. Add small, realistic details such as a wireless keyboard and
```

- ì›ì¹˜ì•ŠëŠ” ì´ë¯¸ì§€ ìƒì„± ìº¡ì…˜
```jsons
negative_prompt = "blurry, low quality, noisy, distorted, deformed, bad proportions, text, watermark, messy, cluttered background, cartoon, anime, painting, sketch"
```

### ì´ë¯¸ì§€ ìƒì„±
```python
# ì´ë¯¸ì§€ ìƒì„±
# lora_settingsì— ìˆëŠ” ëª¨ë“  ì„¤ì • ë‹¤ í•œ ë²ˆì”© ìƒì„±
# ì´ë¯¸ì§€ íŒŒì¼ ì´ë¦„ì€ keyê°’
for setting_name, (lora_names, lora_weights) in lora_settings.items():
    print(f"\n=== {setting_name.upper()} ===")

    # LoRA ì„¸íŒ…
    pipe.set_adapters(lora_names, adapter_weights=lora_weights)

    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()

    start_time = time.time()

    process = psutil.Process(os.getpid())

    # Step 1: Low resolution ìƒì„±
    low_res_image = pipe(
        prompt=generated_prompt,
        negative_prompt=negative_prompt,
        num_inference_steps=40,
        guidance_scale=7.5,
        width=768,
        height=512
    ).images[0]

    # Step 2: ì—…ìƒ˜í”Œ (High Resolution)
    low_res_image = low_res_image.resize((1152, 768), Image.LANCZOS)

    pipe.enable_vae_tiling()

    high_res_image = pipe(
        prompt=generated_prompt,
        negative_prompt=negative_prompt,
        image=low_res_image,
        strength=0.45,
        num_inference_steps=30,
        guidance_scale=7.0
    ).images[0]

    end_time = time.time()

    # ì €ì¥
    image_path = os.path.join(output_dir, f"{setting_name}.png")
    high_res_image.save(image_path)

    # ì„±ëŠ¥ ì¸¡ì •
    if torch.cuda.is_available():
        peak_vram = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB ë‹¨ìœ„
        print(f"ì´ë¯¸ì§€ ìƒì„± ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print(f"ìµœëŒ€ VRAM ì‚¬ìš©ëŸ‰: {peak_vram:.2f} MB")
    else:
        print("CUDA ì‚¬ìš© ë¶ˆê°€: GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
```

### ìƒì„±ëœ ì´ë¯¸ì§€
| BASE_ONLY | OTT_ONLY | 3D_ONLY | BASE_OTT | BASE_3D | OTT_3D | MIX |
|--|--|--|--|--|--|--|
|![base_only](https://github.com/user-attachments/assets/af568760-d98a-4fbf-8628-7ada5981178f)|![ott_only](https://github.com/user-attachments/assets/001529f8-6039-46a2-bb58-7d767820afee)|![3d_only](https://github.com/user-attachments/assets/b6eedec4-182e-425b-a6f8-804f3b6880ef)|![base_ott](https://github.com/user-attachments/assets/e88cc68c-ef04-483c-ac94-014d097804b4)|![base_3d](https://github.com/user-attachments/assets/dba9632e-67c0-4dfb-86cb-46835eb2bcab)|![ott_3d](https://github.com/user-attachments/assets/0f64c6ce-ec67-46e6-bafa-e53d154e5ac3)|![mix](https://github.com/user-attachments/assets/96061e23-b179-4aba-bb59-54aab8e9d423)|


colab : [OTT_multi_modal_ai_test_v1](https://colab.research.google.com/drive/1wB6UdLQTLwUIomIvl5SHK_8xk-joFJFQ?usp=sharing)