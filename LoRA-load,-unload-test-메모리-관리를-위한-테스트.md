ìƒìœ„ ë¬¸ì„œë¡œ ì´ë™ : [AI Wiki](https://github.com/100-hours-a-week/16-Hot6-wiki/wiki/AI-Wiki)

# ğŸ§ª SDXL LoRA Unload Test

Colab í™˜ê²½ì—ì„œ Stable Diffusion XL(SDXL)ì— ë‘ ê°œì˜ LoRAë¥¼ ì ìš©í•œ í›„,  
íŠ¹ì • LoRAë§Œ í•´ì œ(delete)í•˜ëŠ” ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

---

## 1ï¸âƒ£ ì„¤ì¹˜ ë° ì„í¬íŠ¸

```bash
!pip install diffusers transformers safetensors accelerate -U

import torch, gc, os, psutil
from diffusers import StableDiffusionXLPipeline
from safetensors.torch import load_file
from huggingface_hub import hf_hub_download
```

---

2ï¸âƒ£ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸ í•¨ìˆ˜

```bash
def print_memory(tag=""):
    print(f"\nğŸ” [ë©”ëª¨ë¦¬ - {tag}]")
    print(f"CUDA Allocated: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB")
    print(f"RAM Used: {psutil.virtual_memory().used / 1024 ** 3:.2f} GB")
```

---

3ï¸âƒ£ SDXL íŒŒì´í”„ë¼ì¸ ë¡œë“œ
```bash
print_memory("Start")

pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
    variant="fp16",
).to("cuda")

print_memory("After SDXL Load")

ì¶œë ¥ ì˜ˆì‹œ:

ğŸ” [ë©”ëª¨ë¦¬ - Start]
CUDA Allocated: 0.00 MB
RAM Used: 1.83 GB

ğŸ” [ë©”ëª¨ë¦¬ - After SDXL Load]
CUDA Allocated: 6724.67 MB
RAM Used: 2.00 GB
```

---

4ï¸âƒ£ LoRA 2ê°œ ì ìš©
```bash
# ë‹¤ìš´ë¡œë“œ
papercut = hf_hub_download("TheLastBen/Papercut_SDXL", "papercut.safetensors")
lcm = hf_hub_download("latent-consistency/lcm-lora-sdxl", "pytorch_lora_weights.safetensors")

# ì ìš©
pipe.load_lora_weights(papercut, adapter_name="papercut")
pipe.load_lora_weights(lcm, adapter_name="lcm")

ğŸ“Œ ê²½ê³  ë©”ì‹œì§€ (ë¬´ì‹œ ê°€ëŠ¥):

No LoRA keys associated to CLIPTextModel... (ì´ ê²½ê³ ëŠ” ë¬´ì‹œí•´ë„ ë©ë‹ˆë‹¤)
```

---


5ï¸âƒ£ ì–´ëŒ‘í„° ëª©ë¡ í™•ì¸ ë° ì œê±° í…ŒìŠ¤íŠ¸

```bash
print(pipe.get_list_adapters())
# ì¶œë ¥: {'unet': ['papercut', 'lcm']}

pipe.delete_adapters("papercut")

print(pipe.get_list_adapters())
# ì¶œë ¥: {'unet': ['lcm']}
```

---

âœ… ê²°ê³¼ ìš”ì•½

í•­ëª©	ë‚´ìš©
SDXL ë¡œë“œ í›„ CUDA ì‚¬ìš©ëŸ‰	ì•½ 6.7 GB
ì ìš©í•œ LoRA	papercut, lcm
delete_adapters(\"papercut\") í›„	lcmë§Œ ë‚¨ìŒ
ê²°ê³¼	íŠ¹ì • LoRA ì •ìƒì ìœ¼ë¡œ ì œê±°ë¨ âœ…


---

- ì°¸ê³ 
	â€¢	pipe.get_list_adapters() ë¡œ í˜„ì¬ ì ìš©ëœ LoRA ëª©ë¡ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
	â€¢	pipe.delete_adapters("name") ìœ¼ë¡œ íŠ¹ì • ì–´ëŒ‘í„°ë§Œ ì œê±° ê°€ëŠ¥
	â€¢	ì œê±° í›„ torch.cuda.empty_cache() í˜¸ì¶œ ì‹œ VRAMë„ í•´ì œë©ë‹ˆë‹¤

---

- ê´€ë ¨ ë¦¬ì†ŒìŠ¤
	â€¢	[TheLastBen/Papercut_SDXL (Hugging Face)](https://huggingface.co/TheLastBen/Papercut_SDXL)
	â€¢	[latent-consistency/lcm-lora-sdxl](https://huggingface.co/latent-consistency/lcm-lora-sdxl)
	â€¢	[Diffusers Docs â€“ LoRA](https://huggingface.co/docs/diffusers/using-diffusers/lora)





# ì´ ë°©ë²•ì´ ì•ˆë˜ì„œ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ í•´ê²°
## pipe.unload_all_lora() ì¨ì„œ ì „ì²´ì ìœ¼ë¡œ ì§€ì›Œì£¼ê³ , ë‹¤ì‹œ í• ë‹¹í•´ì¤Œ.(OOM í•´ê²° ì™„ë£Œ)
